



So, I finally got beta access to OpenAI's, GPT-3 model API.... and wow, is it cool. I immediately started testing it in their playground by feeding it parts of my blog. I was blown away by how it was able to replicate my prose with only a few paragraphs. Playing with the parameters some, I was able to generate almost a full paragraph before it started getting very repetitive. It was so good that I did insert some of GPT-3's sentences into one of my blogs.

My awe of the results wasn't surprising given the amount of industry buzz GPT-3 created last fall when its public beta was released. The reviews of the model were so strong that it was almost treated as real AI. However, I was a little skeptical given the initial assessment from the guys on the [Machine Learning Street Talk podcast], which essentially said it was not true NLU, rather an NLP model trained on a ridiculous number of parameters (175 B!).  So it is a high parameter model that has great memorization. The next generation will be trained on more and it might not even matter if it's just memorization it will be so good it won't matter.

I was a little taken aback by how expensive OpenAI is to use. I nearly burned through my free us in an hour of the sandbox. Now, the prices should come down the it comes out of beta and there are more competitors in the market. There are several competitors already getting VC funding to do just that. 
